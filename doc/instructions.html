<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Category recognition practical (CNN version)</title>
  <link rel="stylesheet" href="base.css" />
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
  </script>
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<h1 id="image-classification-practical">Image classification practical</h1>
<p>This is an <a href="http://www.robots.ox.ac.uk/~vgg">Oxford Visual Geometry Group</a> computer vision practical, authored by <a href="http://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a> and Andrew Zisserman (Release 2015).</p>
<p><img width=100% src="images/cover.jpeg" alt="cover"/></p>
<p>This practical is on image classification, where an image is classified according to its visual content. For example, does it contain an airplane or not. Important applications are image retrieval - searching through an image dataset to obtain (or retrieve) those images with particular visual content, and image annotatvion - adding tags to images if they contain particular object categories.</p>
<p>The goal of this session is to get basic practical experience with image classification. It includes: (i) training a visual classifier for five different image classes (airplanes, motorbikes, people, horses and cars); (ii) assessing the performance of the classifier by computing a precision-recall curve; (iii) training set and testing set augmentation; and (iv) obtaining training data for new classifiers using Bing image search and using the classifiers to retrieve images from a dataset.</p>
<div class="toc">
<ul>
<li><a href="#image-classification-practical">Image classification practical</a><ul>
<li><a href="#getting-started">Getting started</a></li>
<li><a href="#part-1-training-and-testing-an-image-classifier">Part 1: Training and testing an Image Classifier</a><ul>
<li><a href="#stage-1a-data-preparation">Stage 1.A: Data Preparation</a></li>
<li><a href="#stage-1b-train-a-classifier-for-images-containing-motorbikes">Stage 1.B: Train a classifier for images containing motorbikes</a></li>
<li><a href="#stage-1c-classify-the-test-images-and-assess-the-performance">Stage 1.C: Classify the test images and assess the performance</a></li>
<li><a href="#stage-1d-setting-the-hyper-parameter-c-of-the-svm">Stage 1.D: Setting the hyper-parameter C of the SVM</a></li>
<li><a href="#stage-1e-learn-a-classifier-for-the-other-classes-and-assess-its-performance">Stage 1.E: Learn a classifier for the other classes and assess its performance</a></li>
<li><a href="#stage-1f-vary-the-image-representation">Stage 1.F: Vary the image representation</a></li>
<li><a href="#stage-1g-vary-the-classifier">Stage 1.G: Vary the classifier</a></li>
<li><a href="#stage-1h-vary-the-number-of-training-images">Stage 1.H: Vary the number of training images</a></li>
<li><a href="#stage-1i-data-augmentation">Stage 1.I: Data augmentation</a></li>
</ul>
</li>
<li><a href="#part-2-training-an-image-classifier-for-retrieval-using-bing-images">Part 2: Training an Image Classifier for Retrieval using Bing images</a></li>
<li><a href="#part-3-advanced-encoding-methods">Part 3: Advanced Encoding Methods</a><ul>
<li><a href="#stage-2j-first-order-methods">Stage 2.J: First order methods</a></li>
<li><a href="#stage-2k-second-order-methods">Stage 2.K: Second order methods</a></li>
</ul>
</li>
<li><a href="#links-and-further-work">Links and further work</a></li>
<li><a href="#acknowledgements">Acknowledgements</a></li>
<li><a href="#history">History</a></li>
</ul>
</li>
</ul>
</div>
<h2 id="getting-started">Getting started</h2>
<p>Read and understand the <a href="../overview/index.html#installation">requirements and installation instructions</a>. The download links for this practical are:</p>
<ul>
<li>Code and data: <a href="http://www.robots.ox.ac.uk/~vgg/share/practical-category-cnn-recognition-2015b.tar.gz">practical-category-recognition-cnn-2015b.tar.gz</a> 914MB</li>
<li>Code only: <a href="http://www.robots.ox.ac.uk/~vgg/share/practical-category-recognition-cnn-2015b-code-only.tar.gz">practical-category-recognition-2015b-code-only.tar.gz</a> 12MB</li>
<li>Data only: <a href="http://www.robots.ox.ac.uk/~vgg/share/practical-category-recognition-cnn-2015b-data-only.tar.gz">practical-category-recognition-2015b-data-only.tar.gz</a> 902MB</li>
<li><a href="https://github.com/vedaldi/practical-object-category-recognition">Git repository</a> (for lab setters and developers)</li>
</ul>
<p>After the installation is complete, open and edit the script <code>exercise1.m</code> in the MATLAB editor. The script contains commented code and a description for all steps of this exercise, relative to <a href="#part1">Part I</a> of this document. You can cut and paste this code into the MATLAB window to run it, and will need to modify it as you go through the session. Other files such as <code>exercise2.m</code>, contain the code for other parts of the practical, as indicated below.</p>
<p>Note: the student packages contain only the code required to run the practical. The complete package, including code to preprocess the data, is available on GitHub.</p>
<p><a id='part1'></a></p>
<h2 id="part-1-training-and-testing-an-image-classifier">Part 1: Training and testing an Image Classifier</h2>
<h3 id="stage-1a-data-preparation">Stage 1.A: Data Preparation</h3>
<p>The data provided in the directory data consists of images and pre-computed descriptors for each image. The JPEG images are contained in data/images. The data consists of three image classes (containing airplanes, motorbikes or persons) and`background' images (i.e. images that do not contain these three classes). In the data preparation stage, this data is divided as:</p>
<table>
<thead>
<tr>
<th>--</th>
<th>aeroplane</th>
<th>motorbike</th>
<th>person</th>
<th>background</th>
</tr>
</thead>
<tbody>
<tr>
<td>train</td>
<td>112</td>
<td>120</td>
<td>1025</td>
<td>1019</td>
</tr>
<tr>
<td>test</td>
<td>126</td>
<td>125</td>
<td>983</td>
<td>1077</td>
</tr>
<tr>
<td>total</td>
<td>238</td>
<td>245</td>
<td>2008</td>
<td>2096</td>
</tr>
</tbody>
</table>
<p>An image is represented by a single vector descriptor. Mapping the visual content of an image to a single descriptor vector is often regarded as an encoding step, and the resulting descriptor is sometimes called a code. The main benefit of working with fixed length vectors is that they can be compared by simple vectorial metrics such as Euclidean distance. For the same reason, they are a natural representation to use in learning an image classifier.</p>
<p>We will use a Convolutional Neural Network (CNN) encoding. The process of constructing the CNN descriptor starting from an image is summarized next: </p>
<p><img width=100% src="images/encoding.png" alt="cover"/></p>
<p>First, the network is pre-trained on the ImageNet dataset to classify an image into one of a thousand categories. This determines all the parameters of the CNN, such as the weights of the convolutional filters. Then, for a new image, the trained network is used to generate a descriptor vector from the response of the final fully connected layer of the network with this image as input. For this practical we will use the VGG-M network, which produces a 128 dimensional descriptor vector.</p>
<h3 id="stage-1b-train-a-classifier-for-images-containing-motorbikes">Stage 1.B: Train a classifier for images containing motorbikes</h3>
<p>We will start by training a classifier for images that contain motorbikes. The files <code>data/motorbike_train.txt</code> and <code>data/motorbike_val.txt</code> list images that contain motorbikes. </p>
<blockquote>
<p><strong>Task:</strong> Look through example images of the motorbike class and the background images by browsing the image files in the data directory.</p>
</blockquote>
<p>The motorbike training images will be used as the positives, and the background 
images as negatives. The classifier is a linear Support Vector Machine (SVM). </p>
<blockquote>
<p><strong>Task:</strong> Train the classifier by following the steps in <code>exercise1.m</code>.</p>
</blockquote>
<p>We will first assess qualitatively how well the classifier works by using it to rank all the training images. </p>
<blockquote>
<p><strong>Question:</strong> What do you expect to happen? 
<strong>Task:</strong> View the ranked list using the provided function <code>displayRankedImageList</code> as shown in <code>excercise1.m</code>.</p>
</blockquote>
<p>You can use the function <code>**displayRelevantVisualWords**</code> to visualize the areas of the image that the classifier thinks are most related to the class (see the example embedded in <code>exercise1.m</code>).</p>
<blockquote>
<p><strong>Queston:</strong> Do the words correspond to patches you would expect to be selected?</p>
</blockquote>
<h3 id="stage-1c-classify-the-test-images-and-assess-the-performance">Stage 1.C: Classify the test images and assess the performance</h3>
<p>Now apply the learnt classifier to the test images. Again, you can look at the qualitative performance by using the classifier score to rank all the test images. Note the bias term is not needed for this ranking, only the classification vector w.</p>
<blockquote>
<p><strong>Question:</strong> Why is the bias term not needed?</p>
</blockquote>
<p>Now we will measure the retrieval performance quantitatively by computing a Precision-Recall curve. Recall the definitions of Precision and Recall:</p>
<p><img width=100% src="images/pr1.png" alt="pr1"/></p>
<p>The Precision-Recall curve is computed by varying the threshold on the classifier (from high to low) and plotting the values of precision against recall for each threshold value. In order to assess the retrieval performance by a single number (rather than a curve), the Average Precision (AP, the area under the curve) is often computed.
 Make sure you understand how the precision values in the Precision-Recall curve correspond to the ranking of the positives and negatives in the retrieved results</p>
<h3 id="stage-1d-setting-the-hyper-parameter-c-of-the-svm">Stage 1.D: Setting the hyper-parameter C of the SVM</h3>
<p><em>Skip to <a href="#stage1f">Stage 1.F</a> on fast track</em></p>
<p>If there is a significant difference between the training and test performance, then that indicates over fitting. The difference can often be reduced, and the test performance (generalization), improved by changing the SVM C parameter. </p>
<p>Edit <code>exercise1.m</code> to vary the $C$ parameter in the range 0.1 to 1000 (the default is $C=100$), and plot the AP on the training and test data as C varies.</p>
<p><strong>Note:</strong> hyper-parameters and performance should actually be assessed on a validation set that is held out from the training set. They should not be assessed on the test set. In this practical we are not enforcing this good practice, but don't optimize on the test set once you move on from this practical and start to classify your own data.</p>
<h3 id="stage-1e-learn-a-classifier-for-the-other-classes-and-assess-its-performance">Stage 1.E: Learn a classifier for the other classes and assess its performance</h3>
<p><em>Skip to <a href="#stage1f">Stage 1.F</a> on fast track</em></p>
<p>Now repeat Stage B and C for each of the other two classes: airplanes and people. To do this you can simply rerun <code>exercise1.m</code> after changing the dataset loaded at the beginning in stage (A). Remember to change both the training and test data. In each case record the AP performance measure.</p>
<blockquote>
<p><strong>Question:</strong> Does the AP performance match your expectations based on the variation of the class images?</p>
</blockquote>
<h3 id="stage-1f-vary-the-image-representation">Stage 1.F: Vary the image representation</h3>
<p>An important practical aspect of image descriptors is their normalization. For example, if we regard the CNN descriptor as a discrete probability distribution it would seem natural that its elements should sum to 1. This is the same as normalizing the descriptor vectors in L1 norm. However, in <code>exercise1.m</code> L2 normalization (sum of squares) is used instead.</p>
<blockquote>
<p><strong>Task:</strong> Modify <code>exercise1.m</code> to use L1 normalization and no normalization and measure the performance change.</p>
</blockquote>
<p>A linear SVM can be thought of as using a linear kernel
$$
 K(\mathbf{h},\mathbf{h}') = \sum_{i=1}^d h_i h'_i
$$
to measure the similarity between pair of objects $h$ and $h'$ (in this case pairs of CNN descriptors).</p>
<blockquote>
<p><strong>Question:</strong> What can you say about the self-similarity,$K(\mathbf{h},\mathbf{h})$, of a descriptor $\mathbf{h}$ that is L2 normalized?</p>
</blockquote>
<p>Compare $K(\mathbf{h},\mathbf{h})$ to the similarity, $K(\mathbf{h},\mathbf{h}')$,of two different L2 normalized descriptors $\mathbf{h}$ and $\mathbf{h}'$</p>
<blockquote>
<p><strong>Questions:</strong></p>
<ul>
<li>Can you say the same for unnormalized or L1 normalized descriptors?</li>
<li>Do you see a relation between the classification performance and L2 normalization?</li>
</ul>
</blockquote>
<p>A useful rule of thumb is that better performance is obtained if the vectors that are ultimately fed to a linear SVM (after any intermediate processing) are L2 normalized.</p>
<h3 id="stage-1g-vary-the-classifier">Stage 1.G: Vary the classifier</h3>
<p>Up to this point we have used a linear SVM, treating the descriptors representing each image as vectors normalized to a unit Euclidean norm. Now we will use a Hellinger kernel classifier but instead of computing kernel values we will explicitly compute the feature map, so that the classifier remains linear (in the new feature space). The definition of the Hellinger kernel (also known as the Bhattacharyya coefficient) is
$$
 K(\mathbf{h},\mathbf{h}') = \sum_{i=1}^d \sqrt{h_i h'_i}
$$
where $\mathbf{h}$ and $\mathbf{h}'$ are normalized descriptors. Compare this with the expression of the linear kernel given above: all that is involved in computing the feature map is taking the square root of the descriptor values.</p>
<blockquote>
<p><strong>Questions:</strong></p>
<ul>
<li>Based on the rule of thumb introduced above, how should the descriptors $\mathbf{h}$ and $\mathbf{h}'$ be normalized?</li>
<li>Should you apply this normalization before or after taking the square root?</li>
</ul>
</blockquote>
<p>Now we will again learn the image classifier using the Hellinger kernel instead of the linear one.</p>
<blockquote>
<p><strong>Tasks:</strong></p>
<ul>
<li>Edit <code>exercise1.m</code> so that the square root of the descriptors are used for the feature vectors. Make sure that the proper normalization is used. In practice this involves writing one line of MATLAB code for the training and one for the test descriptors.</li>
<li>Retrain the classifier for the motorbike class, and measure its performance on the test data.</li>
<li>Try the other descriptor normalization options and check that your choice yields optimal performance.</li>
</ul>
<p><strong>Questions:</strong></p>
<ul>
<li>Why is this procedure equivalent to using the Hellinger kernel in the SVM classifier?</li>
<li>Why is it an advantage to keep the classifier linear, rather than using a non-linear kernel?</li>
</ul>
</blockquote>
<p><strong>Note:</strong> when learning the SVM, to save training time we are not changing the $C$ parameter. This parameter influences the generalization error and should be relearnt on a validation set when the kernel is changed (see stage <a href="#stage1d">Stage 1.D</a>). However, in this case the influence of $C$ is small as can be verified experimentally.</p>
<h3 id="stage-1h-vary-the-number-of-training-images">Stage 1.H: Vary the number of training images</h3>
<p><strong>Skip to Part 2 on fast track</strong></p>
<p>Up to this point we have used all the available training images. Now edit <code>exercise1.m</code> and change the fraction variable to use 10% and 50% of the training data.</p>
<blockquote>
<p><strong>Question:</strong> What performance do you get with the linear kernel? And with the Hellinger kernel?
<strong>Question:</strong> Do you think the performance has `saturated' if all the training images are used, or would adding more training images give an improvement?</p>
</blockquote>
<h3 id="stage-1i-data-augmentation">Stage 1.I: Data augmentation</h3>
<p><strong>Skip to Part 2 on fast track</strong></p>
<p>Up to this point the descriptor vector has been computed from a single central crop of the image. We now consider representing each image multiple times  by generating a descriptor for both the original image and for the image after `flipping' (a mirror reflectance about a vertical axis). This data augmentation will be used in a different manner at training and test time. In training, the descriptors from the flipped images will be used as additional training samples (i.e. each original image generates two data samples for training). In testing, the descriptors for the original and flipped image will be averaged resulting in, again, a single vector representing the test image.</p>
<p>Edit <code>exercise1.m</code> to include the training and test augmentation as specified above. Note, the change in classification performance if: (i) only training data augmentation is used, (ii) only testing data augmentation is used; and (iii) both training and test data are augmented.</p>
<blockquote>
<p><strong>Question:</strong> Is classifying the average vector for the test image the same as classifying each vector independently and then averaging the classifying score??
<strong>Question:</strong> When would you expect flipping augmentation to be detrimental to performance?
<strong>Question:</strong> How could additional descriptors be obtained from each image?</p>
</blockquote>
<h2 id="part-2-training-an-image-classifier-for-retrieval-using-bing-images">Part 2: Training an Image Classifier for Retrieval using Bing images</h2>
<p>In Part 1 of this practical the training data was provided and all the feature vectors pre-computed. The goal of this second part is to choose the training data yourself in order to optimize the classifier performance. The task is the following: you are given a large corpus of images and asked to retrieve images of a certain class, e.g. those containing a bicycle. You then need to obtain training images, e.g. using Bing Image Search, in order to train a classifier for images containing bicycles and optimize its retrieval performance.</p>
<p>The MATLAB code <code>exercise2.m</code> provides the following functionality: it uses the images in the directory data/myImages and the default negative list data/background_train.txt to train a classifier and rank the test images. To get started, we will train a classifier for horses:</p>
<p>Use Bing image search with ''horses'' as the text query (you can also set the photo option on)</p>
<p>Pick 5 images and drag and drop (save) them into the directory <code>data/myImages</code>. These will provide the positive training examples.</p>
<blockquote>
<p><strong>Tasks:</strong></p>
<ul>
<li>Run the code <code>exercise2.m</code> and view the ranked list of images. Note, since feature vectors must be computed for all the training images, this may take a few moments.</li>
<li>Now, add in 5 more images and retrain the classifier.</li>
</ul>
</blockquote>
<p>The test data set contains 148 images with horses. Your goal is to train a classifier that can retrieve as many of these as possible in a high ranked position. You can measure your success by how many appear in the first 36 images (this performance measure is `precision at rank-36'). Here are some ways to improve the classifier:</p>
<blockquote>
<p><strong>Tasks:</strong></p>
<ul>
<li>Add more positive training images.</li>
<li>Add more positive training images, but choose these to be varied from those you already have.</li>
</ul>
</blockquote>
<p><strong>Note:</strong> all images are automatically normalized to a standard size, and descriptors are saved for each new image added in the data/cache directory. The test data also contains the category car. Train classifiers for it and compare the difficulty of this and the horse class.</p>
<h2 id="part-3-advanced-encoding-methods">Part 3: Advanced Encoding Methods</h2>
<p><strong>Skip to end on fast track</strong></p>
<p>Up to this point we have used a BoVW representation of the original dense SIFT description of the image, together with spatial information coded by a spatial tiling of the image. The BoVW representation is simply a histogram of the number of SIFT vectors assigned to each visual word. The histogram has K bins, where K is the size of the vocabulary created by K-means, and each bin corresponds to a count of the SIFT vectors lying in the Voronoi cell associated with that visual word.</p>
<p>The BoVW is an encoding of the dense SIFTs into a feature vector. One way to think about the encoding is that it aims to best represent the distribution of SIFT vectors of that image (in a manner that is useful for discriminative classification). BoVW is a hard-assignment of SIFTs to visual words, and alternative soft-assignment methods are possible where the count for a SIFT vector is distributed across several histogram bins. </p>
<p>In this section we investigate a different encoding that records more than a simple count as a representation of the distribution of SIFT vectors. In particular we consider two encodings: one that records first moment information (such as the mean) of the distribution assigned to each Voronoi cell, and a second that records both the first and second moment (e.g. the mean and covariance of the distribution). The MATLAB code for computing these encodings in the file <code>exercise1.m</code>.</p>
<h3 id="stage-2j-first-order-methods">Stage 2.J: First order methods</h3>
<p>The vector of locally aggregated descriptors (VLAD) is an example of a first order encoding. It records the residuals of vectors within each Voronoi cell (i.e. the  difference between the SIFT vectors and the cluster centre (from the k-means) as illustrated below:</p>
<p><img height=200px src="images/vor.png" alt="vor" /></p>
<p>Suppose the SIFT descriptors have dimension $D$, then the total size of the VLAD vector is $K \times D$ (since a D-dimensional residual is recorded for each Voronoi cell). Typically, $K$ is  between 16 and 512, and D is 128 or less.</p>
<blockquote>
<p><strong>Question:</strong> Compare the dimension of VLAD and BoVW vectors for a given value of K. What should be the relation of the K in VLAD to the K in BoVW in order to obtain descriptors of the same dimension?
<strong>Task:</strong> Replace the encoding used in exercise1 with the VLAD encoding, and repeat the classification experiments for the three classes of Part I.</p>
</blockquote>
<p>Note, the performance improvement obtained simply by changing the feature encoding.</p>
<h3 id="stage-2k-second-order-methods">Stage 2.K: Second order methods</h3>
<p>The Fisher Vector (FV) is an example of second order encoding. It records both the residuals (as in VLAD) and also the covariance of the SIFTs assigned to each Voronoi cell. Its implementation uses a Gaussian Mixture Model (GMM) (instead of k-means) and consequently SIFTs are softly assigned to mixture components (rather than a hard assignment as in BoVW). Suppose there are k mixture components and the covariance is restricted to a diagonal matrix (i.e. only the variance of each component is recorded) then the total size of the Fisher vector is $2K \times d$.</p>
<p>Look through the computation of the FV</p>
<blockquote>
<p><strong>Task:</strong> Replace the encoding used in exercise1 with the FV encoding, and repeat the classification experiments for the three classes of Part I.</p>
</blockquote>
<p>Note, the performance improvement obtained (over BoVW and VLAD) </p>
<blockquote>
<p><strong>Question:</strong> What are the advantages or disadvantages of FV compared to VLAD in terms of computation time and storage/memory footprint - especially for a large number (hundreds of millions) of images.</p>
</blockquote>
<p>That completes this practical.</p>
<h2 id="links-and-further-work">Links and further work</h2>
<ul>
<li>The code for this practical is written using the software package <a href="http://www.vlfeat.org">VLFeat</a>. This is a software library written in MATLAB and C, and is freely available as source code and binary.</li>
<li>The images for this practical are taken from the <a href="http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2007/">PASCAL VOC 2007 benchmark</a>.</li>
<li>For a tutorial on large scale image classification and references to the literature, see <a href="https://sites.google.com/site/lsvr13/">here</a>.</li>
</ul>
<h2 id="acknowledgements">Acknowledgements</h2>
<ul>
<li>Funding from ERC grant "Integrated and Detailed Image Understanding", and the EPSRC Programme Grant "SeeBiByte".</li>
</ul>
<p><img height=100px src="images/erc.jpg" alt="erc" /><img height=100px src="images/epsrc.png" alt="epsrc" /></p>
<h2 id="history">History</h2>
<ul>
<li>First used in the Oxford AIMS CDT, 2015-16</li>
<li>Replaces the Image Catgorization practical based on hand-crafted features.</li>
</ul></body>
</html>
